{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Code_Sumbission.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"x8qMl9E77Wkw","colab_type":"text"},"source":["#Section 0: Imports / Installs / Mounting Google Drive / Tensorboard Setup"]},{"cell_type":"code","metadata":{"id":"FFBlLytQ4m17","colab_type":"code","colab":{}},"source":["import os\n","import shutil\n","\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import normalize\n","from collections import Counter\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.transforms import ToTensor\n","from torchvision import transforms\n","import torch.optim as optim"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"llYdMtMJcu5B","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TYIiebGEusua","colab_type":"code","colab":{}},"source":["%load_ext tensorboard\n","from torch.utils.tensorboard import SummaryWriter\n","#be sure to rename filepath as appropriatef\n","BASE_PATH = '/content/drive/My Drive/CIS_522_Project/'\n","ROOT_LOG_DIR = \"{0}logs\".format(BASE_PATH)\n","\n","logger_3a = SummaryWriter(os.path.join(ROOT_LOG_DIR, \"3a\"))\n","logger_3b = SummaryWriter(os.path.join(ROOT_LOG_DIR, \"3b\"))\n","logger_3c = SummaryWriter(os.path.join(ROOT_LOG_DIR, \"3c\"))\n","logger_3d = SummaryWriter(os.path.join(ROOT_LOG_DIR, \"3d\"))\n","logger_4a = SummaryWriter(os.path.join(ROOT_LOG_DIR, \"4a\"))\n","logger_4b = SummaryWriter(os.path.join(ROOT_LOG_DIR, \"4b\"))\n","\n","%tensorboard --logdir {ROOT_LOG_DIR.replace(\" \", \"\\\\ \")}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UlmpLXbLBnCY","colab_type":"text"},"source":["#Section 1: Data Loading / Cleaning / Processing / Creating train test splits"]},{"cell_type":"markdown","metadata":{"id":"pZKn7lT3BQTW","colab_type":"text"},"source":["## 1A. FairFace"]},{"cell_type":"markdown","metadata":{"id":"2F-KKcuyVBcr","colab_type":"text"},"source":["Now we are going to load the Fairface dataset from a zipped file. The images and labels (formatted in .csv) can be downloaded from https://github.com/joojs/fairface, but it is also located in the Google Drive."]},{"cell_type":"code","metadata":{"id":"UUAZq1NdSazh","colab_type":"code","colab":{}},"source":["# unzip the image .zip file, this takes about half an hour\n","#be sure to rename filepath as appropriate\n","!7z x '/content/drive/My Drive/CIS_522_Project/Fairface/fairface.zip'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F6g5u_OES4e8","colab_type":"text"},"source":["Next, we are going to manipulate the file names. This is a tricky part such that without the procedure, there will be a mislabeling of the dataset. Originally the data is ordered like 1.jpg, 2.jpg, ...., 86744.jpg. However, after loading to Colab, the order is changed to 1.jpg, 10,jpg, 100.jpg, ...., 86744.jpg. To handle this, we add zeros to each name so it changes to 00001.jpg, 00002.jpg, 00003.jpg, ...., 86744.jpg and preserve the original order."]},{"cell_type":"code","metadata":{"id":"RBTPGfFKSgiR","colab_type":"code","colab":{}},"source":["# change the image name to make sure the order is correct \n","# Originally we found the order is \"1.jpg, 10,jpg, 100.jpg, ....\", which does not match the order in label file\n","path = '/content/fairface_o/train/train'\n","for filename in os.listdir(path):\n","    num, after = filename.split('.')\n","    num = num.zfill(5)\n","    new_filename = num + \".jpg\"\n","    os.rename(os.path.join(path, filename), os.path.join(path, new_filename))\n","\n","path = '/content/fairface_o/val/val'\n","for filename in os.listdir(path):\n","    num, after = filename.split('.')\n","    num = num.zfill(5)\n","    new_filename = num + \".jpg\"\n","    os.rename(os.path.join(path, filename), os.path.join(path, new_filename))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RXs-mWAaSyfq","colab_type":"text"},"source":["Then we load the labels, which includes gender and ethnicity, and define the imagefolder."]},{"cell_type":"code","metadata":{"id":"fUZiE0WBSqM6","colab_type":"code","colab":{}},"source":["# read label file\n","#be sure to rename filepath as appropriate\n","train_label = pd.read_csv(\"/content/drive/My Drive/CIS_522_Project/Fairface/fairface_label_train.csv\")\n","val_label = pd.read_csv(\"/content/drive/My Drive/CIS_522_Project/Fairface/fairface_label_val.csv\")\n","\n","train_label['race'] = train_label['race'].str.replace('_Hispanic', '')\n","val_label['race'] = val_label['race'].str.replace('_Hispanic', '')\n","train_label['race'] = train_label['race'].str.replace(' ', '')\n","val_label['race'] = val_label['race'].str.replace(' ', '')\n","\n","train_gender = np.argmax(pd.get_dummies(train_label['gender']).to_numpy(),axis=1)\n","val_gender = np.argmax(pd.get_dummies(val_label['gender']).to_numpy(),axis=1)\n","\n","train_race = np.argmax(pd.get_dummies(train_label['race']).to_numpy(),axis=1)\n","val_race = np.argmax(pd.get_dummies(val_label['race']).to_numpy(),axis=1)\n","\n","\n","# create Dataset/Dataloader for training and validation set\n","train_root = '/content/fairface_o/train' \n","val_root = '/content/fairface_o/val' \n","\n","fairface_data_train = ImageFolder(root = train_root,transform = transforms.Compose([transforms.Resize((256,256)), transforms.ToTensor()]))\n","fairface_data_val = ImageFolder(root = val_root,transform = transforms.Compose([transforms.Resize((256,256)), transforms.ToTensor()]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tBxlHBDdSxnQ","colab_type":"text"},"source":["Noted are the numerical labels for ethnicity and gender:\n","\n","Ethnicity:\n","0. Black\n","1. East Asian\n","2. Indian\n","3. Latino\n","4. Middle Eastern\n","5. Southeast Asian\n","6. White\n","\n","Gender:\n","0. Female\n","1. Male"]},{"cell_type":"markdown","metadata":{"id":"GvJDn8wH4qbf","colab_type":"text"},"source":["##1B. IMDB"]},{"cell_type":"markdown","metadata":{"id":"MGuOcRIzPFjJ","colab_type":"text"},"source":["The following code uses the cleaned excel document *imdb.xlsx* as well as a folder of appropriately named images under the filepath, e.g. .../imdb/test/firstname_lastname_0. However, the raw IMDB images and labels (in .mat format) can be found at https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/, under links for \"metadata\" and \"faces only.\""]},{"cell_type":"code","metadata":{"id":"9cXWXTR82pUp","colab_type":"code","colab":{}},"source":["#be sure to rename filepath as appropriate\n","!unzip '/content/drive/My Drive/CIS_522_Project/imdb.zip'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"64boXsoy2pSB","colab_type":"code","colab":{}},"source":["src = '/content/imdb/test'\n","os.remove('/content/imdb/test/.DS_Store.jpg')\n","\n","#be sure to rename filepath as appropriate\n","df = pd.read_excel('/content/drive/My Drive/CIS_522_Project/imdb.xlsx')\n","df = df.sort_values(by=['full_path'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ChRPc2WsXVie","colab_type":"code","colab":{}},"source":["test_gender = np.argmax(pd.get_dummies(df.gender).to_numpy(),axis=1)\n","test_race = np.argmax(pd.get_dummies(df.ethnicity).to_numpy(),axis=1)\n","\n","images = ImageFolder(root = '/content/imdb',transform = transforms.Compose([transforms.Resize((256,256)), transforms.ToTensor()]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lV-RwVi8CEnu","colab_type":"text"},"source":["#Section 2: Dataset / DataLoader Setup "]},{"cell_type":"markdown","metadata":{"id":"3CGQ0UTsVejZ","colab_type":"text"},"source":["We next define the Dataset we will use for both FairFace and IMDB. The dataset contains race and gender information for the data (each image)."]},{"cell_type":"code","metadata":{"id":"OlhYQkwzVTKR","colab_type":"code","colab":{}},"source":["class Face_Dataset(Dataset):\n","    def __init__(self, data, race, gender, transform = None):\n","      self.data = data\n","      self.race = race\n","      self.gender = gender\n","      self.transform = transform     \n","      pass\n","\n","    def __len__(self):\n","      return len(self.data)\n","      \n","    def __getitem__(self, idx):\n","      if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","      item_data = self.data[idx][0]\n","      item_race = self.race[idx]\n","      item_gender = self.gender[idx]\n","\n","      return item_data, item_race, item_gender"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d0NptoorQsTG","colab_type":"text"},"source":["This function visalizes 5 images from a dataset."]},{"cell_type":"code","metadata":{"id":"CfFn-5aH-3hT","colab_type":"code","colab":{}},"source":["def visualize_images(data):\n","  ethnicities = ['Black','East Asian','Indian','Latino','Middle Eastern','Southeast Asian','White']\n","  for i in range(5):\n","    fig = plt.figure()\n","    inputs, gender_label, race_label = data[i][0], data[i][2], data[i][1]\n","    if gender_label == 0:\n","      gender_label = 'Female'\n","    else:\n","      gender_label = 'Male'\n","    \n","    race_label = ethnicities[race_label]\n","\n","    plt.title('Gender is {label1}, Race is {label2}'.format(label2=race_label, label1=gender_label))\n","    plt.imshow(inputs.permute(1,2,0).cpu().numpy(), cmap='gray')\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dsDCLVh2-NKr","colab_type":"text"},"source":["##2A. FairFace"]},{"cell_type":"code","metadata":{"id":"MotXkW-1V6SX","colab_type":"code","colab":{}},"source":["# create the dataset\n","train_data = Face_Dataset(fairface_data_train, train_race, train_gender)\n","val_data = Face_Dataset(fairface_data_val, val_race, val_gender)\n","\n","# construct dataloaders\n","b_size = 128\n","train_loader = DataLoader(train_data, batch_size=b_size, shuffle=True)\n","val_loader = DataLoader(val_data, batch_size=b_size, shuffle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KXp6yEnPV9YV","colab_type":"code","colab":{}},"source":["visualize_images(train_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qlnzXHqU-5PB","colab_type":"code","colab":{}},"source":["visualize_images(val_data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ou-YpOsG-NSZ","colab_type":"text"},"source":["##2B. IMDB"]},{"cell_type":"code","metadata":{"id":"BnBZf42L-L0M","colab_type":"code","colab":{}},"source":["test_data = Face_Dataset(images, test_race, test_gender)\n","test_loader = DataLoader(test_data, batch_size=b_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OR7TeJ-24hdX","colab_type":"code","colab":{}},"source":["visualize_images(test_data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oMW7v9mJCFUm","colab_type":"text"},"source":["#Section 3: Network Model / Network Initialization / Loss and Training / Validation / Testing Loop for Baseline Models"]},{"cell_type":"markdown","metadata":{"id":"gcavyZVeRaR-","colab_type":"text"},"source":["##Training/Testing loops\n","We created generalized training and testing loops that can apply to nearly all of our models."]},{"cell_type":"code","metadata":{"id":"tu5RydYmpA0Q","colab_type":"code","colab":{}},"source":["def test_model(test_dataloader, net, lossfn, fairmasking=False):\n","    total_loss = 0\n","    correct = 0\n","    total = 0  \n","    correct_lst = [0] * 7\n","    total_lst = [0] * 7\n","    net.eval()\n","    with torch.no_grad():\n","\n","        for image, race, gender in test_dataloader:\n","            image, race, gender = image.to(device), race.to(device), gender.to(device)\n","\n","            if fairmasking == True:\n","              output, _ = net(image)\n","            else:\n","              output = net(image)\n","\n","            loss = lossfn(output, gender)\n","            _, pred = torch.max(output.data, 1)\n","\n","            total_loss += loss.item()\n","            total += race.shape[0]\n","\n","            correct += (torch.sum(pred == gender)).item()\n","\n","            for i in range(7):\n","              correct_i = correct_lst[i]\n","              total_i = total_lst[i]\n","              correct_i += (torch.sum((pred == gender) & (race == i))).item()\n","              total_i += (torch.sum(race == i)).item()\n","              correct_lst[i] = correct_i\n","              total_lst[i] = total_i\n","\n","        acc = correct/total\n","        acc0 = correct_lst[0]/total_lst[0]\n","        acc1 = correct_lst[1]/total_lst[1]\n","        acc2 = correct_lst[2]/total_lst[2]\n","        acc3 = correct_lst[3]/total_lst[3]\n","        acc4 = correct_lst[4]/total_lst[4]\n","        acc5 = correct_lst[5]/total_lst[5]\n","        acc6 = correct_lst[6]/total_lst[6]\n","\n","        var = np.var([acc0, acc1, acc2, acc3, acc4, acc5, acc6])\n","\n","        print('Evaluation accuracy on test set: %s, Loss: %s' %(acc, total_loss/total))\n","        print('Unfairness:', var)\n","        print('Gender accuracy|Black:',acc0)\n","        print('Gender accuracy|East Asian:',acc1)\n","        print('Gender accuracy|Indian:',acc2)\n","        print('Gender accuracy|Latino:',acc3)\n","        print('Gender accuracy|Middle Eastern:',acc4)\n","        print('Gender accuracy|Southeast Asian:',acc5)\n","        print('Gender accuracy|White:',acc6)\n","        \n","        return acc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"csJ8esP4WFY9","colab_type":"code","colab":{}},"source":["def train_model(train_dataloader, val_dataloader, net, optimizer, lossfn, logger=None, epochs=10, verbose=True, print_every=10, fairmasking=False):     \n","    net = net.to(device)\n","    net.train()\n","    \n","    step = 0\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n","\n","    for epoch in range(epochs):\n","        total_loss = 0\n","        correct = 0\n","        total = 0\n","\n","        for image, race, gender in train_dataloader:\n","            iamge, race, gender = image.to(device), race.to(device), gender.to(device)\n","            optimizer.zero_grad()\n","            if fairmasking == True:\n","              output, _ = net(image)\n","            else:\n","              output = net(image)\n","\n","            loss = lossfn(output, gender)\n","            loss.backward()\n","            optimizer.step()\n","\n","            _, pred = torch.max(output, 1)                    \n","            total += race.shape[0]\n","            total_loss += loss.item()\n","\n","            correct += (torch.sum(pred == gender)).item()\n","\n","            if ((step % print_every) == 0):\n","                if logger != None:\n","                  info = { ('loss') : loss.item()}\n","                  for tag, value in info.items():\n","                    logger.add_scalar(tag, value, step)\n","\n","                  logger.add_scalar('train accuracy', acc, step)\n","                  val_acc = test_model(val_dataloader, net, lossfn, fairmasking)\n","                  logger.add_scalar('val accuracy', val_acc, step)\n","\n","                if verbose:\n","                    print(\" --- step: %s Acc: %s Loss: %s \" %(step, correct/total, total_loss/total) )\n","            \n","            step += 1\n","\n","        print(\"Training: Epoch: %s, Acc: %s, Loss: %s\" %(epoch, correct/total, total_loss/total))\n","        scheduler.step()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OxTpym1H-g8O","colab_type":"text"},"source":["##3A. Logistic Regression"]},{"cell_type":"code","metadata":{"id":"kGMbLuwuQNf_","colab_type":"code","colab":{}},"source":["class LogisticRegression(nn.Module):\n","    def __init__(self):\n","        super(LogisticRegression, self).__init__()\n","        self.linear = nn.Linear(input_size*input_size*3, output_size)\n","    def forward(self, x):\n","        x = x.view(x.size(0), -1)\n","        out = F.softmax(self.linear(x))\n","        return out "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8S8HMo2HQVJG","colab_type":"code","colab":{}},"source":["# training Logistic baseline\n","input_size = 256\n","output_size = 2\n","learning_rate = 0.001     \n","num_epochs = 30\n","\n","loss_function = nn.CrossEntropyLoss()\n","log_reg = LogisticRegression()\n","optimizer = optim.SGD(log_reg.parameters(), lr = learning_rate)\n","\n","train_model(train_loader, val_loader, log_reg, optimizer, loss_function,  logger_3a, num_epochs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1YO7KStRQbLs","colab_type":"code","colab":{}},"source":["test_model(val_loader, log_reg, loss_function)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6DyxRdFf-hKD","colab_type":"text"},"source":["##3B. FeedForward"]},{"cell_type":"code","metadata":{"id":"GYUTS0c_41t8","colab_type":"code","colab":{}},"source":["input_size = 256\n","output_size = 2\n","\n","class FeedForward(nn.Module):\n","    def __init__(self):\n","        super(FeedForward, self).__init__()\n","        self.fc = nn.Sequential(nn.Dropout(p=0.2),\n","                                nn.Linear(input_size*input_size*3, 500), \n","                                nn.ReLU(), \n","                                nn.BatchNorm1d(500),\n","                                nn.Linear(500, 250), \n","                                nn.ReLU(),                            \n","                                nn.BatchNorm1d(250),\n","                                nn.Linear(250, output_size))\n","\n","    def forward(self, x):\n","        x = x.view(x.size(0), -1)\n","        out= self.fc(x)\n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hHPXmDhH5v66","colab_type":"code","colab":{}},"source":["feed_forward = FeedForward()\n","l2 = 0.001\n","num_epochs = 20\n","loss1 = nn.CrossEntropyLoss()\n","optimizer1 = optim.Adam(feed_forward.parameters(), lr =l2)\n","train_model(train_loader, val_loader, feed_forward, optimizer, loss_function,  logger_3b, num_epochs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eW8ZpcYvSSO5","colab_type":"code","colab":{}},"source":["test_model(val_loader, feed_forward, loss_function)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WP800h1H-hRs","colab_type":"text"},"source":["##3C. CNN"]},{"cell_type":"code","metadata":{"id":"MS99JkixWWUB","colab_type":"code","colab":{}},"source":["# Defining the model\n","class View(nn.Module):\n","    def __init__(self,o):\n","        super().__init__()\n","        self.o = o\n","\n","    def forward(self,x):\n","        return x.view(-1, self.o)\n","    \n","class CNN(nn.Module):\n","    def __init__(self, cin=3, c1=16, c2=32, c3=128):\n","        super().__init__()\n","        d = 0.5\n","\n","        def convbn(ci,co,ksz,s=1,pz=0):\n","            return nn.Sequential(\n","                nn.Conv2d(ci,co,ksz,stride=s,padding=pz),\n","                nn.BatchNorm2d(co),\n","                nn.ReLU(True),\n","                )\n","\n","        self.m = nn.Sequential(\n","            convbn(cin,c1,4,2,1),\n","            nn.Dropout(0.1),\n","            nn.MaxPool2d(kernel_size = 2), \n","            convbn(c1,c2,4,2,1),\n","            nn.Dropout(0.1),\n","            nn.MaxPool2d(kernel_size = 2), \n","            convbn(c2,c3,3,1,1),\n","            nn.AvgPool2d(2),\n","            )\n","        self.f = nn.Sequential(nn.Linear(2048*4, 2)\n","                                    )\n","    def forward(self, x):\n","        x = self.m(x)\n","        x = x.view(x.size(0),-1)\n","        out = self.f(x)\n","        return out "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KB42zy-aWi3a","colab_type":"text"},"source":["We train the CNN using Adam optimizer with default learning rate (1e-03) and 10 epochs. We also set a scheduler for the optimizer, multiply learning rate by 0.9 after each epoch. As a result, The training accuracy achieves 89.55% and validation accuracy is 85.63%."]},{"cell_type":"code","metadata":{"id":"D-tAUHk2Wbhd","colab_type":"code","colab":{}},"source":["cnn = CNN()\n","learning_rate = 0.001\n","optimizer = optim.Adam(cnn.parameters(), lr = learning_rate)\n","loss_function = nn.CrossEntropyLoss()\n","num_epochs = 10\n","\n","train_model(train_loader, val_loader, cnn, optimizer, loss_function,  logger_3c, num_epochs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3L1wsiE8WpZ3","colab_type":"code","colab":{}},"source":["test_model(val_loader, cnn, loss_function)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"enIHDyXMSROF","colab_type":"text"},"source":["##3D. ResNet"]},{"cell_type":"code","metadata":{"id":"M78qOLtTW_Di","colab_type":"code","colab":{}},"source":["def conv3x3(in_channels, out_channels, stride = 1):\n","    return nn.Conv2d(in_channels, out_channels, kernel_size= 3, stride = stride, \n","                     padding = 1, bias = False)\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = conv3x3(in_channels, out_channels, stride)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace = True)\n","        self.conv2 = conv3x3(out_channels, out_channels)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.downsample = downsample\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        if self.downsample:\n","            residual = self.downsample(x)\n","        out += residual\n","        out = self.relu(out)\n","        \n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"keMeDb2TXBv2","colab_type":"code","colab":{}},"source":["class ResNet(nn.Module):\n","    def __init__(self, block, layers, num_classes = 7):\n","        super(ResNet, self).__init__()\n","        self.in_channels = 16\n","        self.conv = conv3x3(3, 16)\n","        self.bn = nn.BatchNorm2d(16)\n","        self.relu = nn.ReLU(inplace = True)\n","        self.layer1 = self.make_layer(block, 16, layers[0])\n","        self.layer2 = self.make_layer(block, 32, layers[0], 2)\n","        self.layer3 = self.make_layer(block, 64, layers[1], 2)\n","        self.avg_pool = nn.AvgPool2d(8)\n","        self.fc = nn.Linear(4096, num_classes)\n","    \n","    def make_layer(self, block, out_channels, blocks, stride = 1):\n","        downsample = None\n","        if (stride !=1) or (self.in_channels != out_channels):\n","            downsample = nn.Sequential(\n","                conv3x3(self.in_channels, out_channels, stride = stride),\n","                nn.BatchNorm2d(out_channels))\n","        layers = []\n","        layers.append(block(self.in_channels, out_channels, stride, downsample))\n","\n","        self.in_channels = out_channels\n","        for i in range(1, blocks):\n","            layers.append(block(out_channels, out_channels))\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = self.conv(x)\n","        out = self.relu(self.bn(out))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.avg_pool(out)\n","        out = out.view(out.size(0), -1)\n","        out = self.fc(out)\n","\n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ryWZR5HZXGe1","colab_type":"code","colab":{}},"source":["# net_args current ResNet34\n","net_args = {\n","    \"block\": ResidualBlock,\n","    \"layers\": [3, 4, 6, 3]\n","}\n","\n","resnet = ResNet(**net_args)\n","learning_rate = 0.0001\n","optimizer = optim.Adam(resnet.parameters(), lr = learning_rate)\n","loss_function = nn.CrossEntropyLoss()\n","num_epochs = 30\n","\n","train_model(train_loader, val_loader, resnet, optimizer, loss_function,  logger_3d, num_epochs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-dJ4mPxXQpD","colab_type":"code","colab":{}},"source":["test_model(val_loader, resnet, loss_function)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lhzx9OANCFeu","colab_type":"text"},"source":["#Section 4: Network Model / Network Initialization / Loss and Training / Validation / Testing Loop for Fair Models"]},{"cell_type":"markdown","metadata":{"id":"gv9fzMPo-x4X","colab_type":"text"},"source":["##4A. Fair regularization of ResNet"]},{"cell_type":"markdown","metadata":{"id":"vJaPeqyi4__L","colab_type":"text"},"source":["### Penalty function and weight decay\n","\n","We regularize by putting penalties on differences on loss/accuracy among different races. We basically use $L^2$ penalties, i.e.\n","$$\n","penalty_{total} = \\sum_{i=1}^{7} (averageloss_{race_i} - averageloss_{all})^2\n","$$.\n","\n","We also use weight decay througout the training, e.g. we started from weight $\\epsilon = 1$ in the first epoch and decay it to 0.001 at the the of the training."]},{"cell_type":"code","metadata":{"id":"8YODdNMu5H31","colab_type":"code","colab":{}},"source":["def loss_penalty(total_loss, black_loss, eastasian_loss, indian_loss,\n","                 latino_loss, middleeastern_loss, southeastasian_loss, white_loss):\n","    loss = (total_loss - black_loss)**2 + (total_loss - eastasian_loss)**2 + (total_loss- indian_loss)**2 + (total_loss-latino_loss)**2 + 10*(total_loss - middleeastern_loss)**2 + (total_loss - southeastasian_loss)**2 + (total_loss - white_loss)**2\n","    loss *= 10\n","    return loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gtbFlLpPrznt","colab_type":"code","colab":{}},"source":["# Penalty function on accuracy difference across features\n","def acc_penalty(total_acc, black_acc, eastasian_acc, indian_acc,\n","                 latino_acc, middleeastern_acc, southeastasian_acc, white_acc): \n","    loss = (total_acc - black_acc)**2 + (total_acc - eastasian_acc)**2 + (total_acc- indian_acc)**2 + (total_acc-latino_acc)**2 + 10*(total_acc - middleeastern_acc)**2 + (total_acc - southeastasian_acc)**2 + (total_acc - white_acc)**2\n","    loss *= 10\n","    return loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iwjlq9aP5M-M","colab_type":"code","colab":{}},"source":["# loss decay\n","epsilon_start = 1\n","epsilon_end = 0.01\n","def decay_rate(steps, total_epoch):\n","    return epsilon_start - (steps/(total_epoch*1400)) * (epsilon_start - epsilon_end)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LaPOXjXfq0Ru","colab_type":"text"},"source":["###Training loop"]},{"cell_type":"code","metadata":{"id":"CeSVeWujqzp6","colab_type":"code","colab":{}},"source":["def train_penalized_model(train_dataloader, val_dataloader, net, optimizer, lossfn, logger=None, epochs = 20, verbose = True, print_every=100, mode=\"loss\"):\n","    step = 0\n","\n","    net.to(device)\n","    net.train() \n","\n","    # for 5 epochs\n","    for epoch in range(epochs):\n","        total_loss = 0\n","    \n","        correct = 0\n","        total = 0\n","\n","        correct_lst = [0] * 7\n","        total_lst = [0] * 7\n","        \n","        for data, race, gender in train_dataloader:\n","            loss_total_lst = [0] * 7\n","\n","            data, race, gender = data.to(device), race.to(device), gender.to(device)\n","\n","            optimizer.zero_grad()\n","            output = net(data)\n","\n","            _, pred = torch.max(output, 1)\n","            \n","            correct += (torch.sum(pred == gender)).item()\n","            total += race.shape[0]\n","\n","            for i in range(7):\n","              correct_i = correct_lst[i]\n","              total_i = total_lst[i]\n","              loss_tot_i = loss_tot_lst[i]\n","\n","              correct_i += (torch.sum((pred == gender) & (race == i))).item()\n","              total_i += (torch.sum(race == i)).item()\n","              loss_i = lossfn(torch.index_select(output, 0, 1*(race == 0)), torch.index_select(gender,0, 1*(race == 0)))\n","              loss_total_i += loss_i.item()\n","              \n","              correct_lst[i] = correct_i\n","              total_lst[i] = total_i\n","              loss_total_lst[i] = loss_tot_i\n","\n","\n","            acc = correct/total\n","            loss = lossfn(output, gender)\n","\n","            total_loss_avg = loss.sum().item()/gender.shape[0]\n","            \n","            if mode == \"loss\":\n","                penalty = loss_penalty(total_loss_avg, loss_total_lst[0]/correct_lst[0], loss_total_lst[1]/correct_lst[1], loss_total_lst[2]/correct_lst[2],loss_total_lst[3]/correct_lst[3], loss_total_lst[4]/correct_lst[4], loss_total_lst[5]/correct_lst[5], loss_total_lst[6]/correct_lst[6])\n","            else:\n","                penalty = acc_penalty(acc, correct_lst[0]/total_lst[0], correct_lst[1]/total_lst[1], correct_lst[2]/total_lst[2],correct_lst[3]/total_lst[3], correct_lst[4]/total_lst[4], correct_lst[5]/total_lst[5], correct_lst[6]/total_lst[6])\n","\n","            # decay the penalty\n","            penalty *= decay_rate(step,epochs)\n","            loss += penalty\n","            \n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","            if ((step % print_every) == 0):\n","                if logger != None:\n","                  info = { ('loss') : loss.item()}\n","                  for tag, value in info.items():\n","                    logger.add_scalar(tag, value, step)\n","                    \n","                  logger.add_scalar('train accuracy', acc, step)\n","                  val_acc = test_model(val_dataloader, net, lossfn, fairmasking)\n","                  logger.add_scalar('val accuracy', val_acc, step)\n","                if verbose:\n","                    print(\" --- step: %s Acc: %s Loss: %s \" %(step, correct/total, total_loss/total) )\n","\n","            step += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qAIsS0-Ym2dt","colab_type":"code","colab":{}},"source":["net_args = {\n","    \"block\": ResidualBlock,\n","    \"layers\": [3, 4, 6, 3]\n","}\n","\n","resnet_reg = ResNet(**net_args).to(device)\n","loss_function = nn.CrossEntropyLoss()\n","learning_rate = 0.001\n","optimizer = torch.optim.Adam(resnet_reg.parameters(), lr = learning_rate)\n","num_epochs = 30\n","\n","train_penalized_model(train_loader, val_loader, resnet_reg, optimizer, loss_function, logger_4a, num_epochs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w0h0l08QlV5l","colab_type":"code","colab":{}},"source":["test_model(val_loader, resnet_reg, loss_function)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Jm9f5ctx-0kx"},"source":["##4B. Fair masking"]},{"cell_type":"markdown","metadata":{"id":"3wruCeHl2C0Y","colab_type":"text"},"source":["In this section, we propose to use fair masking to mitigate the unfairness of neural networks in Fairface dataset. The general idea of masking is that for a network(e.g. CNN, ResNet), we will choose one hidden layer to add fairness regularization. To do this we will output the values of all features in this hidden layer, mask out those that are strongly dependent with the protected feature. In this framework, it can be tricky to quantify the dependence between hidden features and protected features since the former is continuous while the latter is discrete in most cases (e.g. gender, ethnicity). Actually, there aren't any widely-used dependence test between a continuous variable and a non-binary categorical varible.\n","\n","To conduct this dependence test, we propose two approaches: Correlation-based dependence test and logistic regression based dependence test.\n"]},{"cell_type":"markdown","metadata":{"id":"-uBGNbN6aPTr","colab_type":"text"},"source":["### Model structure"]},{"cell_type":"code","metadata":{"id":"UYTJ1zVKZ7lc","colab_type":"code","colab":{}},"source":["# implement two sequential networks together\n","# redefine the networks now :)\n","class Seq_CNN(nn.Module):\n","    def __init__(self, cin=3, c1=16, c2=32, c3=128):\n","        super().__init__()\n","        d = 0.5\n","\n","        def convbn(ci,co,ksz,s=1,pz=0):\n","            return nn.Sequential(\n","                nn.Conv2d(ci,co,ksz,stride=s,padding=pz),\n","                nn.BatchNorm2d(co),\n","                nn.ReLU(True),\n","                )\n","        \n","        self.mask = torch.tensor(np.ones(32))\n","        self.latent_feature = torch.tensor(np.zeros(32))\n","\n","        self.seq_1 = nn.Sequential(\n","            convbn(cin,c1,4,2,1),\n","            nn.Dropout(0.1),\n","            nn.MaxPool2d(kernel_size = 2), #64*64\n","            convbn(c1,c2,4,2,1),\n","            nn.Dropout(0.1),\n","            nn.MaxPool2d(kernel_size = 2), #16*16\n","            convbn(c2,c3,3,1,1),\n","            nn.AvgPool2d(2),\n","            )\n","        \n","        self.f = nn.Linear(2048*4, 32)\n","        \n","        self.seq_2 = nn.Sequential(\n","            nn.Linear(32, 500),\n","            nn.BatchNorm1d(500),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(500, 2)\n","        )\n","\n","\n","    def forward(self, x):\n","        # first part\n","        x_h = self.seq_1(x)\n","        x_h = x_h.view(x_h.size(0),-1)\n","        latent_feature = self.f(x_h).to(device)\n","        mask = latent_feature * self.mask.to(device)\n","        # second part\n","        out = self.seq_2(mask.float())\n","        return out, latent_feature\n","    \n","    # drop out some features\n","    def Penalize(self, index):\n","        for i in range(len(self.mask)):\n","          if i in index:\n","            self.mask[i] = 0\n","        return None\n","\n","    def Recover(self):\n","        for i in range(len(self.mask)):\n","            self.mask[i] = 1\n","        return None"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SP8fMjG9aVZy","colab_type":"code","colab":{}},"source":["seq_cnn = Seq_CNN()\n","learning_rate = 0.001\n","optimizer = optim.Adam(seq_cnn.parameters(), lr = learning_rate)\n","loss_function = nn.CrossEntropyLoss()\n","num_epochs = 10\n","\n","train_model(train_loader, val_loader, seq_cnn, optimizer, loss_function, logger_4b, num_epochs, fairmasking=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M-Gou6cGUMr_","colab_type":"code","colab":{}},"source":["test_model(val_loader, seq_cnn, loss_function, fairmasking=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OVUnaUEDaj5Q","colab_type":"text"},"source":["### Correlation-based Masking"]},{"cell_type":"markdown","metadata":{"id":"Th7bGu1k2To8","colab_type":"text"},"source":["After training, we can apply rank the 7 ethnicity groups according to their gender classification accuracy on validation set. Then we can calculate the correlation between each feature and the ranking of ethnicity. For those that has correlation larger than prespecified threshold, we will mask out them by setting values to be 0."]},{"cell_type":"code","metadata":{"id":"8bqqVT_aaJbw","colab_type":"code","colab":{}},"source":["def pearsonr(x, y):\n","    mean_x = torch.mean(x)\n","    mean_y = torch.mean(y)\n","    xm = x.sub(mean_x)\n","    ym = y.sub(mean_y)\n","    r_num = xm.dot(ym)\n","    r_den = torch.norm(xm, 2) * torch.norm(ym, 2)\n","    return r_num / r_den"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WaUhCWnNaWi2","colab_type":"code","colab":{}},"source":["def penalize_model_cor(train_dataloader, net, threshold):\n","    # at the end add penalty\n","    total = train_gender.shape[0]\n","    hidden_feature_count = torch.zeros([total,32])\n","    protected_count = torch.zeros(total)\n","    count = 0\n","    Order = np.array([6,5,3,2,0,4,3])\n","    for i, (data, race, gender) in enumerate(train_dataloader):\n","            data, race, gender = data.to(device), race.to(device), gender.to(device)\n","            output,hidden_feature = net(data)\n","\n","            for j in range(data.shape[0]):\n","              H_feature = Variable(hidden_feature[j],requires_grad = False)\n","              hidden_feature_count[count,:] = H_feature\n","              Race = Order[race[j].cpu().int()]\n","              protected_count[count] = Race\n","              count = count + 1\n","    # penalize\n","    index = []\n","    for i in range(32):\n","      cor = pearsonr(protected_count,hidden_feature_count[:,i])\n","      torch.FloatTensor.abs_(cor)\n","      if np.abs(cor) > threshold:\n","        index.append(i)\n","    net.Penalize(torch.tensor(index))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5YnsG3efalWm","colab_type":"code","colab":{}},"source":["# threshold can be set from 0 ~ 0.14\n","threshold = 0.14\n","penalize_model_cor(train_loader, seq_cnn, threshold)\n","test_model(val_loader, seq_cnn, loss_function, fairmasking=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JXpCJNgPc-lt","colab_type":"code","colab":{}},"source":["seq_cnn.Recover()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3GUD-D3PbIZi","colab_type":"text"},"source":["### Logistic regression based Masking"]},{"cell_type":"markdown","metadata":{"id":"08j24eIG2WEr","colab_type":"text"},"source":["\n","Another approach we propose is to fit a logistic regression using hidden features as regressors and ethnicity as outcome variable. Since there are 7 classes in total, there will be 6 coefficients for each of the hidden features. We calculate the difference between maximum and minimum coefficient values for each feature. We regard this value as a measure of dependence between the corresponding hidden feature and protected feature (ethnicity), since if the feature is completely independently with protected feature, then all 6 coefficients should be the same (equal to 1). The larger the difference, the more different contributions the hidden feature makes to predict each class (of protected feature). We rank all the hidden features according to this difference. We can then set threshold to mask out the top $k$ features."]},{"cell_type":"code","metadata":{"id":"l-38IYfLbMw_","colab_type":"code","colab":{}},"source":["def penalize_model_lr(train_dataloader, net, threshold):\n","    total = train_gender.shape[0]\n","    hidden_feature_count = np.zeros([total,32])\n","    protected_count = np.zeros(total)\n","    count = 0\n","    for i, (data, race, gender) in enumerate(train_dataloader):\n","            data, race, gender = data.to(device), race.to(device), gender.to(device)\n","            output,hidden_feature = net(data)\n","\n","            for j in range(data.shape[0]):\n","              H_feature = Variable(hidden_feature[j],requires_grad = False)\n","\n","              hidden_feature_count[count,:] = H_feature.cpu().numpy()\n","              protected_count[count] = race[j].float().to(device)\n","              count += 1\n","\n","    hidden_feature_count = normalize(hidden_feature_count, axis=0)\n","    clf_lr = LogisticRegression(penalty = 'l2', max_iter = 10000, tol=1e-4, random_state=24, solver = 'lbfgs')\n","    clf_lr.fit(hidden_feature_count, protected_count)\n","    \n","    # report the training and test accuracy\n","    training_accuracy = clf_lr.score(hidden_feature_count, protected_count)\n","    pred = clf_lr.predict(hidden_feature_count)\n","\n","    print ('training accuracy is ' + str(training_accuracy))\n","    params = clf_lr.coef_\n","    mn = np.amin(clf_lr.coef_,axis = 0)\n","    mx = np.amax(clf_lr.coef_,axis = 0)\n","    delta = mx - mn\n","    # rank the hidden features\n","    sortd = np.argsort(-delta)\n","\n","    # penalize threshold = 3 is good\n","    index = []\n","    for i in range(threshold):\n","        index.append(sortd[i])\n","    \n","    net.Penalize(torch.tensor(index))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gS_PVo-2bbXX","colab_type":"code","colab":{}},"source":["# can set threshold from 1 to 32\n","threshold = 5\n","penalize_model_lr(train_loader,seq_cnn,threshold)\n","test_model(val_loader,seq_cnn,loss_function, fairmasking=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BvktDR85YX2s","colab_type":"text"},"source":["# Section 5: Transfer Learning"]},{"cell_type":"markdown","metadata":{"id":"zbMpcOLrcpDZ","colab_type":"text"},"source":["The following code tests the IMDb data on the pretrained resnet model from part 3D."]},{"cell_type":"code","metadata":{"id":"c-uRw2PicJLJ","colab_type":"code","colab":{}},"source":["loss_function = nn.CrossEntropyLoss()\n","test_model(test_loader, resnet, loss_function)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iUGulX5nCFoX","colab_type":"text"},"source":["#Section 6: Analysis of Outputs"]},{"cell_type":"markdown","metadata":{"id":"3i--HtdxTq-j","colab_type":"text"},"source":["This section documents the extreme class imbalance of ethnicity in the IMDb dataset, and slight class imbalance of gender, by plotting grouped bar charts for each feature."]},{"cell_type":"code","metadata":{"id":"4_CMkHHhXdnz","colab_type":"code","colab":{}},"source":["labels = ['Black','EastAsian','Indian','Latino','MiddleEastern','SoutheastAsian','White']\n","\n","tr = Counter(train_label.race).most_common()\n","v = Counter(val_label.race).most_common()\n","te = Counter(test_label.ethnicity).most_common()\n","\n","x = np.arange(len(labels))\n","\n","train_freq = [0] * 7\n","val_freq = [0] * 7\n","test_freq = [0] * 7\n","\n","for i in range(7):\n","    tr_tup = tr[i]\n","    v_tup = v[i]\n","    te_tup = te[i]\n","    \n","    train_freq[labels.index(tr_tup[0])] = (tr_tup[1]/86744) * 100\n","    val_freq[labels.index(v_tup[0])] = (v_tup[1]/10954) * 100\n","    test_freq[labels.index(te_tup[0])] = (te_tup[1]/347701) * 100\n","\n","# the label locations\n","width = 0.30  # the width of the bars\n","\n","fig, ax = plt.subplots(figsize=(15,10))\n","rects1 = ax.bar(x - width, train_freq, width, label='FairFace (train)')\n","rects2 = ax.bar(x, val_freq, width, label='FairFace (val)')\n","rects3 = ax.bar(x + width, test_freq, width, label='IMDB (test)')\n","\n","# Add some text for labels, title and custom x-axis tick labels, etc.\n","ax.set_title('Frequency (%) of ethnicity label in each dataset',fontsize=25)\n","ax.set_xticks(x)\n","ax.set_xticklabels(labels)\n","ax.legend(fontsize=18)\n","ax.tick_params(labelsize=15)\n","\n","\n","def autolabel(rects):\n","    for rect in rects:\n","        height = rect.get_height()\n","        height = round(height, 2)\n","        ax.annotate('{}'.format(height),\n","                    xy=(rect.get_x() + rect.get_width() / 2, height),\n","                    xytext=(0, 3),  # 3 points vertical offset\n","                    textcoords=\"offset points\",\n","                    ha='center', va='bottom')\n","\n","autolabel(rects1)\n","autolabel(rects2)\n","autolabel(rects3)\n","\n","fig.tight_layout()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n5oPG5r3Xdrk","colab_type":"code","colab":{}},"source":["labels = ['Female','Male']\n","\n","tr = Counter(train_label.gender).most_common()\n","v = Counter(val_label.gender).most_common()\n","te = Counter(test_label.gender).most_common()\n","\n","x = np.arange(len(labels))\n","\n","train_freq = [0] * 2\n","val_freq = [0] * 2\n","test_freq = [0] * 2\n","\n","for i in range(2):\n","    tr_tup = tr[i]\n","    v_tup = v[i]\n","    te_tup = te[i]\n","    \n","    train_freq[labels.index(tr_tup[0])] = (tr_tup[1]/86744) * 100\n","    val_freq[labels.index(v_tup[0])] = (v_tup[1]/10954) * 100\n","    test_freq[labels.index(te_tup[0])] = (te_tup[1]/347701) * 100\n","\n","# the label locations\n","width = 0.30  # the width of the bars\n","\n","fig, ax = plt.subplots(figsize=(6,7.5))\n","rects1 = ax.bar(x - width, train_freq, width, label='FairFace (train)')\n","rects2 = ax.bar(x, val_freq, width, label='FairFace (val)')\n","rects3 = ax.bar(x + width, test_freq, width, label='IMDB (test)')\n","\n","# Add some text for labels, title and custom x-axis tick labels, etc.\n","ax.set_title('Frequency (%) of gender label in each dataset',fontsize=15)\n","ax.set_xticks(x)\n","ax.set_xticklabels(labels)\n","ax.legend(fontsize=12)\n","ax.tick_params(labelsize=10)\n","\n","\n","def autolabel(rects):\n","    for rect in rects:\n","        height = rect.get_height()\n","        height = round(height, 2)\n","        ax.annotate('{}'.format(height),\n","                    xy=(rect.get_x() + rect.get_width() / 2, height),\n","                    xytext=(0, 3),  # 3 points vertical offset\n","                    textcoords=\"offset points\",\n","                    ha='center', va='bottom')\n","\n","autolabel(rects1)\n","autolabel(rects2)\n","autolabel(rects3)\n","\n","fig.tight_layout()\n","\n","plt.show()"],"execution_count":0,"outputs":[]}]}