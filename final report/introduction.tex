\section{Introduction}
Deep learning has infiltrated into various fields with countless applications. However, many of these tasks are expected to be operated with awareness of fairness â€” lawfully and without discrimination. Consider an example: an HR employee is training a deep neural network to decide whether or not an applicant should be hired. They need be cautious to make sure the decision should not be related to gender and race. As a result, the training process of the neural network involves a trade-off between fairness (non-discrimination) and quality of decision.

In general, having class balance across sensitive groups is critical in the creation of a fair algorithm. In the case of deep learning, one answer to this "trade-off" is to train a neural network while preserving group fairness is by adding a penalty term in loss function, which regularizes the fairness of the algorithms. A more extensive list of different metrics and bias mitigation algorithms has been compiled by IBM in their AI Fairness 360 toolkit \cite{aif360-oct-2018}.

Automatic facial recognition algorithms, which are used by developers including social media companies and federal law enforcement, have been shown to be biased with respect to race/ethnicity, gender, and age. A NIST study evaluated 189 software algorithms from 99 developers and found 10-100 times higher rates of false classification for (East) Asian and Black faces relative to Caucasian ones. Examining only algorithms created in the U.S., the highest rate of misclassification of ethnicity was of Native Americans. Women were also less likely to be correctly identified than men, and older adults were misclassified up to 10 times more than the middle-aged  \cite{grother2019face}. Clearly, there is significant room for improvement when it comes to eliminating bias from automatic facial recognition algorithms.