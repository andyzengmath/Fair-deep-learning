\section{Discussion}

In this project, we have examined the algorithmic fairness of deep neural networks in facial classification tasks. We first set up non-deep learning and deep learning baseline algorithms and observe non-negligible unfairness in gender prediction. In particular, the validation accuracy for the Middle Eastern ethnicity group is consistently at least 10$\%$ higher than that of the Black ethnicity group. By proposing two novel approaches, fair regularization and fair masking, we illustrate that it is possible to reduce the unfairness while maintaining (or even improving upon) the high performance of the baselines. Compared to the baselines, these two novel fair algorithms show improvement of fairness without losing much validation accuracy. We further investigate the performance of the algorithms in transfer learning.

In the IMDb-Faces dataset, we observe that a ResNet pretrained in FairFace shows significant reduced performance (62.81\%) compared to the FairFace validation set (86.71\%). We believe potential reasons include significantly imbalanced distribution of ethnicity and gender groups, mislabelling, and the fact that faces in IMDb-Faces are not cropped as closely as those in FairFace dataset. Notably, the UF measure for the transfer learning on IMDb was by far the lowest, compared to all other models. This may have to do with the fairness-performance trade-off, where lower accuracy is associated with lower unfairness. In order to increase the accuracy of IMDb, we suggest using IMDb data to tune the hyperparameters of a model, instead of FairFace.

We struggled to find a dataset that contained interesting protected features as well as names of subjects, so that identity classification based on facial recognition could be performed. While FairFace has gender and ethnicity well-balanced amongst its dataset, it does not have IDs or names for the individuals in the photograph, or multiple photographs per individual. IMDb-Faces has those names and photographs, but a highly imbalanced and mislabelled dataset, and we had to webscrape its ethnicity data.

Due to the limited time, there are still ideas haven't been investigated throughout the project study. We are curious about the performance and unfairness of DenseNet, another popular network structure in computer vision. We also wish to learn more about fairness in transfer learning: whether a fair model appears to be fairer than baselines after domain transfer can also be an exciting question in this area of research. We also would like to test more protected features, such as age, and explore areas of intersectionality between multiple features (e.g. examining female vs. male classification accuracy across ethnicities).

Regarding the topic of fairness in deep learning, much room is remained for future investigation and study as a whole. First, the effect of hyperparameters in algorithmic fairness hasn't been well-studied  or documented yet. We believe it would be interesting and useful to investigate and provide insight into that how to design a network structure to mitigate potential bias. For example, how does the depth and width of the network influence fairness? How about optimizers, learning rate as well as batch size? Furthermore, existing works only consider the case when only single feature is protected. How to control unfairness of the algorithms regarding multiple protected features can be even more challenging but meaningful. 